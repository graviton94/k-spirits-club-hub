import requests
from bs4 import BeautifulSoup
import sys
import os
import json
import random
import time
import argparse
from pathlib import Path
from datetime import datetime
from urllib.parse import urlencode

# Force UTF-8 for Windows
sys.stdout.reconfigure(encoding='utf-8')
sys.stderr.reconfigure(encoding='utf-8')

# ì„¤ì •
FAIL_LOG = Path('scripts/image_fail_log.txt')
# Defaults (Backward Compatibility)
DEFAULT_INPUT_DIR = Path('data/enriched')
DEFAULT_OUTPUT_FILE = Path('data/enriched/ready_for_confirm.json')

# User-Agent ë¦¬ìŠ¤íŠ¸ (ì°¨ë‹¨ ë°©ì§€ìš©)
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
    "Mozilla/5.0 (iPhone; CPU iPhone OS 17_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Mobile/15E148 Safari/604.1"
]

def build_advanced_search_url(name_en, distillery):
    """êµ¬ê¸€ ê³ ê¸‰ ê²€ìƒ‰ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°í•©í•˜ì—¬ íƒ€ê²ŸíŒ…ëœ URL ìƒì„±"""
    query = f"{name_en} {distillery}"
    base_url = "https://www.google.com/search"
    
    params = {
        "as_st": "y",
        "as_q": query,               # í•„ìˆ˜ í‚¤ì›Œë“œ
        "as_oq": "bottle OR packaging", # ë˜ëŠ” í¬í•¨
        "as_eq": "glass interior",      # ì œì™¸ í‚¤ì›Œë“œ
        "udm": "2",                     # ì´ë¯¸ì§€ ê²€ìƒ‰ ëª¨ë“œ
        "tbs": "isz:m",                 # ì¤‘ê°„ í¬ê¸° ì´ìƒ
        "hl": "ko"                      # í•œêµ­ì–´ ê²°ê³¼ ìš°ì„  (í•„ìš”ì‹œ enìœ¼ë¡œ ë³€ê²½ ê°€ëŠ¥)
    }
    
    return f"{base_url}?{urlencode(params)}"

import re

def fetch_image_url(name_en, distillery):
    """HTML ë‚´ì˜ JSON ë¸”ë¡ ë° URL íŒ¨í„´ì„ ë¶„ì„í•˜ì—¬ ì‹¤ì œ ì´ë¯¸ì§€ URL ì¶”ì¶œ"""
    url = build_advanced_search_url(name_en, distillery)
    headers = {"User-Agent": random.choice(USER_AGENTS)}
    
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        
        html = response.text
        
        # 1. ì›ë³¸ ì†ŒìŠ¤ ë° ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì‹œë„
        # Google JSON êµ¬ì¡°: ["URL", height, width] íŒ¨í„´ íƒìƒ‰
        patterns = re.findall(r'\[\"(https?://[^\"\s]+\.(?:jpg|jpeg|png|webp))\",(\d+),(\d+)\]', html)
        
        found_url = None
        for img_url, h, w in patterns:
            h, w = int(h), int(w)
            
            # í•„í„° 1: ê°€ë¡œê°€ ì„¸ë¡œë³´ë‹¤ ê¸´(Landscape) ì´ë¯¸ì§€ëŠ” ìˆ ë³‘ ì‚¬ì§„ìœ¼ë¡œ ë¶€ì í•©í•˜ë¯€ë¡œ ì œì™¸
            if w > h:
                continue
                
            # í•„í„° 2: íŠ¹ì • ë„ë©”ì¸ ì œì™¸ ë° ê¸¸ì´ ê²€ì‚¬
            if 'gstatic.com' not in img_url and 'google' not in img_url:
                if len(img_url) > 20: 
                    found_url = img_url
                    break
                    
        # 2. ì •ì  img íƒœê·¸ íŒŒì‹± (Fallback 1) - ë¹„ìœ¨ì„ ì•Œ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ìµœì†Œí•œì˜ í•„í„°ë§ë§Œ ìˆ˜í–‰
        if not found_url:
            soup = BeautifulSoup(html, 'html.parser')
            images = soup.find_all('img')
            for img in images:
                src = img.get('src') or img.get('data-src') or img.get('data-deferred-src')
                # ì—¬ê¸°ì„œëŠ” ë¹„ìœ¨ íšë“ì´ ì–´ë ¤ìš°ë¯€ë¡œ ê¸°ì¡´ ë¡œì§ ìœ ì§€
                if src and src.startswith('http') and 'gstatic.com' not in src and 'google' not in src:
                    found_url = src
                    break
                    
        # 3. gstatic ì¸ë„¤ì¼ì´ë¼ë„ ë§¤ì¹­ (Fallback 2)
        if not found_url:
            # ì¸ë„¤ì¼ ì¤‘ì—ì„œë„ ë¹„ìœ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ìˆëŠ” ê²½ìš° í•„í„°ë§ ì‹œë„
            for img_url, h, w in patterns:
                if 'encrypted-tbn' in img_url and int(w) <= int(h):
                    found_url = img_url
                    break
            
        return found_url
        
    except Exception as e:
        print(f"âš ï¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ({name_en}): {e}")
        return None

def main():
    parser = argparse.ArgumentParser(description='Fetch images for enriched spirits data')
    parser.add_argument('--input', help='Input JSON file path')
    parser.add_argument('--output', help='Output JSON file path')
    args = parser.parse_args()

    # Load Data
    all_enriched = []
    
    if args.input:
        input_path = Path(args.input)
        if not input_path.exists():
            print(f"âŒ Input file not found: {input_path}")
            return
        with open(input_path, 'r', encoding='utf-8') as f_in:
            data = json.load(f_in)
            if isinstance(data, list):
                all_enriched.extend(data)
            else:
                print("âŒ Input JSON must be a list of objects")
                return
        output_path = Path(args.output) if args.output else DEFAULT_OUTPUT_FILE
    else:
        # Backward Compatibility: Scan directory
        print(f"ğŸ“‚ Scanning default directory: {DEFAULT_INPUT_DIR}")
        batch_files = list(DEFAULT_INPUT_DIR.glob('whisky_enriched_batch_*.json'))
        if not batch_files:
            print("âŒ No batch files found.")
            return
        for f_path in batch_files:
            with open(f_path, 'r', encoding='utf-8') as f_in:
                all_enriched.extend(json.load(f_in))
        output_path = DEFAULT_OUTPUT_FILE

    print(f"ğŸ” Loaded {len(all_enriched)} items. Starting Image Search...")

    processed_count = 0
    total_items = len(all_enriched)

    for i, item in enumerate(all_enriched):
        # Already has valid image?
        if item.get('imageUrl') and item['imageUrl'].startswith('http') and 'google' not in item['imageUrl']:
            continue
            
        name_en = item.get('metadata', {}).get('name_en', item['name'])
        distillery = item.get('distillery', '')
        
        print(f"ğŸ“¸ [{i+1}/{total_items}] Fetching: {name_en}...")
        
        img_url = fetch_image_url(name_en, distillery)
        
        if img_url:
            item['imageUrl'] = img_url
            item['thumbnailUrl'] = img_url
            item['status'] = 'READY_FOR_CONFIRM'
            item['updatedAt'] = datetime.now().isoformat()
            print(f"âœ… Success: {img_url[:60]}...")
        else:
            item['imageUrl'] = None
            item['status'] = 'IMAGE_FAILED'
            # Log failure
            with open(FAIL_LOG, 'a', encoding='utf-8') as f_fail:
                f_fail.write(f"{item['id']} | {name_en} | {datetime.now().isoformat()}\n")
            print(f"âŒ Failed (Logged): {name_en}")

        processed_count += 1
        
        # Delay
        delay = random.uniform(3, 6) # Slightly faster for batch processing as batches are small
        if i < total_items - 1:
            time.sleep(delay)
            
        # Intermediate Save (only if not single batch file, or just save always)
        # For batch mode, we just save at the end usually, but safe to save here.

    # Save Result
    output_path.parent.mkdir(parents=True, exist_ok=True)
    with open(output_path, 'w', encoding='utf-8') as f_out:
        json.dump(all_enriched, f_out, indent=2, ensure_ascii=False)
        
    print(f"âœ¨ Validation Ready: {output_path}")

    # Final Summary
    print("\n" + "="*50)
    print(" ğŸ“Š [SUMMARY] Image Search (Advanced)")
    print("-" * 50)
    print(f"  â€¢ Total Processed    : {total_items:,}")
    print(f"  â€¢ Images Found       : {sum(1 for i in all_enriched if i.get('imageUrl')):,}")
    print(f"  â€¢ Failed/No Image    : {sum(1 for i in all_enriched if not i.get('imageUrl')):,}")
    print(f"  â€¢ Output File        : {output_path}")
    print("=" * 50 + "\n")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nğŸ‘‹ Aborted by user.")
